# Apache Sparkとは？

Apache Sparkは大規模データ処理のための高速かつ汎用性の高いエンジンです。
<br>Apache HadoopのMapReduceより使いやすいAPIと優れたメモリ管理を実現することを目標としています(いました？)。

Sparkの特徴は、以下の4つがあげられることが多いです。

1. 大規模データを高速処理
1. 開発、利用が簡単
1. 汎用性が高い
1. 実行環境の選択の幅広さ

### 1. 大規模データを高速処理

Sparkでは、メモリを効率的に使用することで、HadoopのMapReduce処理と同等の分散処理を最大100倍程度まで高速化できるビッグデータ処理に適した仕組みです。MapReduceではデータ処理ごとに書き出し処理を行っていたため再利用の際のオーバーヘッドがかかっていたのを、Sparkではメモリに展開したデータを再利用するようにしているようです。

### 2. 開発・利用が簡単

開発者が分散処理を意識しないで済むように、複数台にわたる処理を抽象化することで、1台を対象とした処理を実装するのと同じ感覚で実装が可能になっています。
<br>また、開発言語もScala、Java、Python、Rに対応しているため、新規にプログラミング言語を覚えることなく使用することができます。
(これはデータアナリスト前提の考え方だけど、データ分析するならPythonやRといったイメージはあるので間違いではないか)

### 3. 汎用性が高い

Hadoopのエコシステムでは、ストリーム処理、バッチ処理、機械学習、SQL処理、グラフ処理などの機能を実現するために、別コンポーネントを実装連携する必要がありましたが、Sparkではそれぞれの機能がSparkのライブラリとして提供されているので、単一プラットフォームで利用でき、ライブラリ間の連携も容易です。

### 4. 実行環境の選択の幅広さ

Sparkはスタンドアロン、YARN、Mesosなどの実行環境や、
HDFS、Cassandra、HBase、Hiveなど様々な外部のデータソースも簡単にアクセスできるので、
実行環境の選択時の自由度も高いのが特徴です。

## Sparkの各コンポーネント

### Spark Streaming

Spark StreamingはSpark上でストリーム処理を実装するライブラリで、様々なデータソースから高速にストリームデータを取得できます。
HDFS、Kafka、Flume、Twitter、ZeroMQのデータソースから直接読み込むことができ、自分でカスタムしたデータソースを定義することも可能なため、広い用途に対してストリーム処理を行うことができます。

#### Spark Streaming API

StreamingContextとDStreamの抽象化APIです。

#### StreamingContext

Spark StreamingライブラリのエントリーポイントでSparkクラスタに接続するための以下の機能を提供します。

- Sparkクラスタへの接続
- データストリームの抽象化のインスタンスを生成するためのメソッドの提供

#### DStream(Discretized Stream)

Spark Streamingの抽象化を提供する機能で、ストリーミングデータソースや既存のDStreamに変換処理を行うことで生成されます。
DStreamの生成はStreamingContextクラスで定義されているメソッドをもとに行われるため、DStreamから直接生成することはできません。

…つまるところ、ストリーミングデータを抽象化したオブジェクトで、データソースに対する処理の層が積みあがっているオブジェクト、ってことでいいのかな？
5章、7章でやるそうなので、要確認。

#### Spark Streamingの特徴

ストリームデータを非常に細かい固定時間間隔のデータに分割されたデータをマイクロバッチと呼びます。各マイクロバッチで生成されたデータは1つのRDDとしてストアされるため、以降は通常のRDDに対するSparkコアの処理が実行可能になります。

この特徴を見る感じ、DStreamはストリーム自体に対しての変換処理の記録を保持していて、細かい処理に関しては別途マイクロバッチで実施する感じかな？(そしてウマイクロバッチはウィンドウ処理で生成されたものってことでいいのかな？)

### MLlib

機械学習と統計分析機能を有する分析ライブラリです。主要な機械学習アルゴリズムをサポートしており、モデルの生成・選択・チューニング、複数のアルゴリズムを1つのデータフローとして組み合わせるパイプライン機能も実装されています。

#### MLlib API

DataFrameベースのAPIである、spark.mlパッケージがメイン。

#### パイプライン機能

分析の処理プロセスを構築、評価、チューニングまでを行う以下の一連のワークフローを制御するパイプラインという機能があり、SparkのMLlibにはDataFrame乗で構築されるML Piplineという機能が存在します。

- データのリード
- 分析用のデータ前処理と準備
- 特徴量の抽出
- データをトレーニング、検証、テスト用に分割
- トレーニングのデータセットを用いてモデルの学習
- クロスバリデーションを用いてのモデルのチューニング
- テストデータセットを用いてのモデルの評価
- モデルのデプロイ

#### Note

MLlibにはDaraFrameベースの`org.apache.spark.ml`とRDDベースの`org.apache.spark.mllib`の2種類のパッケージが存在します。
DataFrameは高次の抽象化APIでRDDは低次の抽象化APIとなっていますが、RDDベースのパッケージは2018年時点で新規機能追加が行われていないためDataFrameベースのパッケージをメインに学習を進めていくほうが望ましい。

### Spark SQL

Spark SQLはDataFrame APIをベースにしたクエリ処理を実行するためのSparkライブラリで、以下の特徴があります。

- 標準SQLを利用して様々なデータにアクセスが可能
    - HDFS上のファイル(CSV、JSON、Parquet、Avro、ORCなど)
    - Hiveテーブル
    - RDBMS(JDBC経由でアクセス)

また、ほかのSparkコンポーネントと連携することで、ストリーム処理や機械学習処理に対して標準SQLを活用できるようになります。

Spark SQLはSparkSessionとDataFrameという主要な抽象化で構成されていて、この2つを利用することでデータ処理が可能になります。

#### SparkSession

SparkSessionはSQLContextとHiveContextの代替となるエントリーポイントで、SparkSessionを利用することで、SQLContextとHiveContextの両方を1つのエントリポイントとして使用できます。

#### DataFrame

DataFrameはRDDと比べ抽象化の進んだAPIで、RDBのテーブルに似た構造になっています。(カラム名と型が決められたデータセット)
<br>構造化データを処理するための様々なメソッドが提供されています。

Structured Streaming APIがSpark2.2.0から正式サポートされており、ストリーム処理をSpark SQL上に構築することができるようになりました。

### Spark GraphX

Spark GraphXは大容量のグラフ構造データに対して分散グラフ処理を行えるライブラリです。ソーシャルグラフなどのように要素同士のつながりをグラフで表現が可能になります。

用語としていろいろ上がっているので、メモしておきます。
<br>サブグラフ生成、頂点併合、近接集約、ページランク、強連結成分分解

#### GraphX API

GraphXはRDDベースのAPIでグラフのデータ特性を簡単に生成するために、通常のデータから頂点と辺のRDD生成をし、グラフ処理が可能になります。

#### Note

グラフのデータ特性

- 頂点と辺の集まり
- 辺は方向を持つことが可能
- 頂点と辺は属性を持つことが可能

2016年にGraphXとDataFrameを統合したGraphFramesというライブラリが登場していますが、2018年時点でSparkにマージされていないため外部ライブラリとなっています。GraphFramesが採用されると、より抽象度が高く複雑なグラフ処理が簡単になっていくと思われます。
<br>(ざっと確認しましたが2021年2月現在も取り込まれていないようにみえます。)

### Sparkコア

Sparkコアは分散処理の基本機能として、

- タスクのスケジューリング
- メモリ管理
- ストレージシステムとの連携
- 障害復旧処理
- セキュリティ制御

などを担っています。

#### Spark API

Spark APIはSparkContext、SparkConf、データセットの3つの抽象化機能で構成されています。

##### SparkContext

Sparkで操作を行う際のエントリーポイントであり、以下の役割を持ちます。

- Sparkクラスタの接続
- Spark APIで提供される重要なオブジェクトの生成

Sparkアプリケーションでは、最低一つのSparkContextクラスのインスタンスを生成しなければなりません。
なお、複数インスタンスを使うことも可能ですが、推奨されません。

##### SparkConf

Sparkアプリケーションの構成を定義するためのクラスです。

##### データセットのAPI

Sparkでデータを扱う際に、各ノードに分散しているデータを単一オブジェクトとして扱うためのAPIであり、以下の役割を持ちます。

- 分散しているデータをひとまとまりとして扱う機能
- データの変換、集計、ファイルへの保存などの基本操作を行う機能

Sparkでは、RDD、DataFrame、Datasetという3種類のデータセットのAPIが利用可能です。
<br>RDDはSparkリリース初期からサポートされていたAPIで、DataFrameは構造化データを扱いやすくするためにSpark1.3からサポートされたAPIです。DatasetはRDDとDataFrameの両方のいい部分を取り込むダメにSpark1.6から試験的に導入されSpark2.0で正式サポートされました。

###### RDD(Resilient Distributed Dataset)

Sparkで直接データ処理をするデータセットです。以下の特徴があります。

- 不変性(イミュータブル)
- 耐故障性
- 分散(複数サーバにデータを分散可能)

DataFrameやDatasetの登場で、ユーザーが直接認識する機会は減ったものの、Sparkでの最終的な処理はRDDが用いられているため重要なデータセットです。

**不変性(イミュータブル)**

何らかの処理を実行した場合は新たなRDDが生成されるため、一度生成されたRDDは変更されず、不変性が担保されます。

**耐故障性**

Sparkでは、データ変換のプロセスを記録しており、障害発生時に障害が発生した処理ポイントから変換プロセスを再実行することによりデータ復旧が可能になります。

**分散**

特に分散処理を意識せずにローカル上で処理を行っているのと変わらない操作性を体感できます。

**その他の特徴**

型安全：ベースはScalaなので、コンパイル時にデータの型をチェックすることで実行前に型の不一致を発見できます。
<br>RDDの処理：変換(フィルタ、マップなど)とアクション(コレクト、最大値など)の2種類があります。データの変換処理はアクションが実行されるまで実行されません(遅延処理)。

RDDには、レコードに対して処理をするたびにシリアライゼーション、デシリアライゼーションが発生し処理のオーバーヘッドが大きくなる、という課題があります。

RDDの処理はSpark DAG(Directed Acyclic Graph: 有向非循環グラフ)スケジューラと呼ばれるスケジューラを利用し、効率よく処理が行われるように設計されています。(DAGは逆流することがない有向グラフと考えればよい。)SparkにおいてRDDを頂点として、変換処理が有向辺とすると、一連の変換処理をDAGとみなすことができます。
<br>アクションを実行する際に、DAGを先頭から順に処理していきますが、この時にそのまま頭から実行していくのではなく、処理の実行プランを立てて自動的に最適化してから実行されます。これにより、不要なデータを作成しなくてすむ、変換処理+アクションの処理を最適化、DAGからRDDの再構成ができるため耐故障性が高くなる、などのメリットが得られます。

###### DataFrame

DataFrameはRDBのテーブルのようにカラムに名前と型を持つ構造化されたデータセットです。DataFrame自体はPythonやRにも存在しますが、分散データに対応できるかどうかの違いがあります。
<br>DataFrameは構造化データとして扱いやすいためクエリ処理の実装が容易になります。

RDDと比べたときには、以下の2点で性能の向上が図られています。

- HWのリソース管理を最適化すること(Project Tungsten)
- クエリ実行計画を最適化すること(Catalyst Optimizer)

欠点として、RDDに比べてユーザ定義関数(UDF)が使いづらく、型安全ではない点が挙げられます。

###### Dataset

DataFrameにRDDが持つ型安全とオブジェクト指向プログラミング機能を拡張したインターフェースがDatasetです。双方の優れた点を取り込んでおり、さらにエンコーダというオフ・ヒープ・データと効率的にやり取りを行うためのバイトコードを生成し、オブジェクト全体をデシリアライズすることなく個々の属性へのオンデマンドアクセスできる絹が実装されています。

Note：Datasetでデータを上利する際にはクラスの型をStringで定義する必要がある。また、マップ処理を行う場合はCatalyst Optimizerを使用できない。

###### データセットAPIの簡易比較


| 機能               | RDD | DataFrame | Dataset |
| :----------------- | :-: | :-------: | :-----: |
| 柔軟性             | 〇  |    △     |   〇    |
| 操作性             | △  |    〇     |   〇    |
| 対応データソース数 | ×  |    〇     |   〇    |
| 型安全             | 〇  |    ×     |   〇    |
| メモリ効率         | ×  |    〇     |   〇    |
| 最適化             | ×  |    〇     |   〇    |
| サポート言語       | 〇  |    〇     |   △    |
