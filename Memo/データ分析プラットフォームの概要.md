# データ分析プラットフォームの概要

## データ分析プラットフォームの要件

データ分析プラットフォームとは、
- データを収集・蓄積して分析を行い、事業の効率化・拡大につながるアクションを起こすためのシステム

AI, IoT, ビッグデータの時代に対応していくために、下記3要件を柔軟かつ迅速に実現することが求められる
- 様々な分析手法を用いたビッグデータの分類・整理・分析機能
- 大量かつ多種多様なデータソースの収集機能
- ビッグデータの収集・蓄積・処理機能

(ほぼ同じことを言ってる気がする)

### ビッグデータの分類・整理・分析機能

分析をするために、<br>
大量のデータの中から分析に必要なデータを取り出し、<br>
加工・変換をする<br>
ということを一定の時間内で終わらせられるような機能が必要。

### 多種多様なデータソースの収集機能

IoTの普及による様々なデータを的確に収集する機能が必要(…らしい)。

(いつどんなデータが有用になるかはわからないので、事前に大量にデータを収集しておくのは有意義なことだとは思う。)

### ビッグデータの収集・蓄積・処理機能

大容量データを格納できる機能がないと収集も分析もそもそもできないよねって話。

まとめると、

- 収集
- 蓄積
- 抽出&分析

を高速に行うことができる機能が必要です、ということ。

## データ分析プラットフォームのデータ処理

データ処理の流れは

- データソースからデータを収集(IoTデバイスだけでなく既存のRDBなどのデータも含む)
- 収集したデータを複数台のサーバに分散して蓄積する(ビッグデータは1台のサーバで対応できるサイズではないので分散処理をする)
- 蓄積した様々なデータを分析可能な形に整形・加工する(データフォーマットの統一、欠損値や不正値の処理など)
- 分析(分析者が容易に実装・実行できるよな形で最新の分析手法が取り込まれているとよい)

Sparkだと対応できるらしい。

## データ処理の各機能

### データ収集

様々なデータソースからデータ蓄積処理やストリーム処理へ渡す役割を担う。

- 多様化するデータソースの処理(データ形式や通信方法など)
- 多量のデータソースから流入するデータの収集
- データ収集するタイミングの処理

等の機能が必要。

よく使われるツールとして以下のものがあるらしい。

- [Fluentd](https://www.fluentd.org/)(ログ集約ツール)
- [Apache NiFi](https://nifi.apache.org/)(データフローオーケストレーションツール)
- [Apache Kafka](https://kafka.apache.org/)(メッセージングシステム)

#### 技術トピック

##### データフローオーケストレーション

システム間のデータフローを定義、管理、運用を自動化するツールのこと。
スケーラブルであることも特徴としてはある？

##### メッセージングシステム

いわゆるデータのキューイングシステム？
データを一時的に保持する(キューイング)するシステムを間に挟むことで、非同期処理にすることができ、システム間を疎結合にすることができる。

### データ蓄積

データ蓄積機能では、大容量で多様なデータソースを扱いやすくるための機能が必要になる。

- 大量・多種類のデータの蓄積(分散ファイルシステム、メタデータなど)
- 多様なデータ蓄積デバイスへの対応(クラウド、インメモリ、ディスクなど)
- 過去から直近までのデータを連携および活用

よく使われるのは、HDFS(Hadoop Distributed File System)やAmazon S3など。
<br>また、高速処理につなげる目的でCassandraやHBaseなども用いられるらしい。

#### 技術トピック

##### データのシリアライゼーション

生成したデータを複数サーバ間でやり取りする際に、効率的に転送、処理するためにシリアライゼーションが用いられるらしい。
メモリ上のオブジェクトを外部(ディスクや別サーバ)に出力する際の変換が、分散処理によってデータのやり取りが多いためたくさん発生する…という話？

##### バイナリ データフォーマット

データ量が少ないうちは、CSVやJSONなどの人が理解しやすいデータフォーマットでも問題なかったが、総データ量が大きいビッグデータの処理を行う際には処理速度や転送を高速化するためにデータサイズを抑える必要が出てくる。そのためにバイナリフォーマットを用いている。
<br>Avro, Thrift, ProtocolBufferなどがよく利用されているらしい。

##### カラムナ型ストレージフォーマット

行指向のデータフォーマットはオンライントランザクションに向いていたが、大量のデータを扱う集計処理には向かないので列指向のデータフォーマットが必要となった。
<br>列指向のデータフォーマットは列ごとにデータの型が同一であるため圧縮効率も優れている。そのためビッグデータの集計に向いたデータフォーマットとしてよく用いられている。
<br>ORCFileやParquetなどのフォーマットがよく用いられている。

### データ処理

データの処理方法として大まかに2種類存在する。

- ストリーム処理
- バッチ処理

ストリーム処理はデータの蓄積と並行してリアルタイムに分析を行うための処理で、以下の機能が求められる。

- 流入し続けるデータに対する短時間のデータ処理
- 様々なデータソースに対応可能な簡易分析

SparkだとSpark StreamingやSpark SQL内のStructured Streaming APIが用意されている模様。

バッチ処理は要件に応じて実装方法がかなり異なるため、様々なデータ処理を柔軟に行える必要がある。特に最近求められている機能として以下のものがある。

- 大量データを迅速に的確に取り出すためのデータ変換
- 分析の多様化に伴う多様化するデータの加工処理
- 多様なデータソースの処理

(ストリーム処理とバッチ処理の違いはなんとなく分かるけど、Spark内で収まる範囲でのバッチ処理がピンと来ていない)

#### 技術トピック

##### ウィンドウ処理

ストリーム処理のストリームデータは途切れなく流れてくるが、一定の区間で区切る機能をウィンドウ処理という。
<br>これによってまとまったデータに対しての集計や演算を実現できるようになる。

### データ分析

データの分析手法の選定は分析の利用用途により異なるため、分析用の処理も手法に応じて変化する。
<br>様々な分析手法を単一プラットフォームで実現することが望まれるため、以下の機能が重要となる。

- 多種多様な分析処理
    - 言語の多様化：R, Pythonなど
    - 分析アルゴリズムの多様化：統計分析、機械学習、ディープラーニングなど
- 分析に必要なデータだけの迅速な検索・取得処理
- ビッグデータ処理

## Sparkとは

Sparkではデータ分析プラットフォームで必要とされている機能のうち、データ処理とデータ分析に特化している。また、大量のデータ処理に対しても高速かつ実装が容易である、という点で注目されている。

Sparkとは、The Apache Software Foundationのプロジェクトの1つでOSSの`インメモリ型汎用分散処理エンジン`。
インメモリであるためHadoopのMapReduce処理と比べて最大100倍程度の性能向上が見込める。

対応言語は、ネイティブ言語のScalaとAPIとしてJava、Python、Rが使用可能。







